{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试看一下GGCNN2\n",
    "其实单从学习代码的角度来看，这块没必要写一个notebook，直接在py文件里面改就行了，但是这个东西我想结合着论文看一下，好好研究一下网络的思路和一些比较复杂网络的构建。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GG-CNN2 is a fully convolutional network based on the semantic segmentation architecture from Yu and Koltun (2016), which uses dilated convolutional layers to provide improved performance in semantic segmentation tasks.\n",
    "\n",
    "GG-CNN2是一个基于语义分割模型（[Yu and Koltun（2016）](https://arxiv.org/abs/1511.07122)）的全卷积神经网络，它使用膨胀卷积层来提高语义分割任务的性能。而之前所使用的GGCNN仅仅是一个baseline而已。\n",
    "\n",
    "GGCNN2和GGCNN有着同样的输入输出，只是采用了一些其他的测试来提升速度，跟一些直接从其他地方照搬过来进行语义分割的CNN相比，GGCNN和GGCNN2有着体量小，速度快的特点。网络结构调节的思路主要是在第8节，看了一下之后，发现调优工作主要是在jacquard数据集上来的，刚我也申请了这一数据集，正在下载，最晚后天能下载完吧。\n",
    "\n",
    "另外读完了之后，我觉得这个也没有太多技巧可言，好像就是在试，边试边修建，把那些发挥作用不大的都裁剪掉了，这样速度就上来了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GGCNN2(nn.Module):\n",
    "    def __init__(self, input_channels=1, filter_sizes=None, l3_k_size=5, dilations=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if filter_sizes is None:\n",
    "            filter_sizes = [16,  # First set of convs\n",
    "                            16,  # Second set of convs\n",
    "                            32,  # Dilated convs\n",
    "                            16]  # Transpose Convs\n",
    "\n",
    "        if dilations is None:\n",
    "            dilations = [2, 4]\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # 4 conv layers.\n",
    "            nn.Conv2d(input_channels, filter_sizes[0], kernel_size=11, stride=1, padding=5, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(filter_sizes[0], filter_sizes[0], kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(filter_sizes[0], filter_sizes[1], kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(filter_sizes[1], filter_sizes[1], kernel_size=5, stride=1, padding=2, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Dilated convolutions.\n",
    "            nn.Conv2d(filter_sizes[1], filter_sizes[2], kernel_size=l3_k_size, dilation=dilations[0], stride=1, padding=(l3_k_size//2 * dilations[0]), bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(filter_sizes[2], filter_sizes[2], kernel_size=l3_k_size, dilation=dilations[1], stride=1, padding=(l3_k_size//2 * dilations[1]), bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Output layers\n",
    "            nn.ConvTranspose2d(filter_sizes[2], filter_sizes[3], 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(filter_sizes[3], filter_sizes[3], 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.pos_output = nn.Conv2d(filter_sizes[3], 1, kernel_size=1)\n",
    "        self.cos_output = nn.Conv2d(filter_sizes[3], 1, kernel_size=1)\n",
    "        self.sin_output = nn.Conv2d(filter_sizes[3], 1, kernel_size=1)\n",
    "        self.width_output = nn.Conv2d(filter_sizes[3], 1, kernel_size=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "\n",
    "        pos_output = self.pos_output(x)\n",
    "        cos_output = self.cos_output(x)\n",
    "        sin_output = self.sin_output(x)\n",
    "        width_output = self.width_output(x)\n",
    "\n",
    "        return pos_output, cos_output, sin_output, width_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了直观上看这个定义模型的算法更长了些之外，里面跟GGCNN至少有两点不同：\n",
    "- 1.可以供更改的参数只有三个，一个是kernel size，一个是l3_k_size，还有一个就是dilation，其他参数都定死了，应该是测试后得到的比较好的结果。\n",
    "- 2.这里把中间的运算层“打了个包”，定义成为了一个Squential序列模型，命名为特征提取层，这边会输出总的结果，最后再定义几个不同数据的反卷积输出层，这样做的好处一个是数据打包层次分明，模型大的时候不容易搞混，还有一个就是，可以看到，模型内的定义是不需要给每一层独立命名的，这就省了不少事，起码我觉得省了不少事\n",
    "\n",
    "然后其他就没啥了，不出意外的话，我直接在原先的程序里面啥都不改，模型改调用这个，就是可以正常训练的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
